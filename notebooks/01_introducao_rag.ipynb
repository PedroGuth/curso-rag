{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# üöÄ **M√≥dulo 1: Introdu√ß√£o ao RAG - O Superpoder da IA**\n",
       "\n",
       "> *Porque √†s vezes a IA precisa consultar os \"livros\" antes de responder*\n",
       "\n",
       "---\n",
       "\n",
       "## **Aula 1.1: O que √© RAG e por que √© tipo ter um Google na cabe√ßa da IA?**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas o que √© RAG?**\n",
       "\n",
       "RAG √© como ter um **estudante super inteligente** que, antes de responder qualquer pergunta, consulta uma biblioteca gigante de documentos. √â tipo ter um Google na cabe√ßa da IA!\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Um estudante consultando livros antes de responder\n",
       "\n",
       "**Por que RAG √© importante?**\n",
       "\n",
       "Imagine que voc√™ pergunta para o ChatGPT: \"Qual foi o resultado do √∫ltimo jogo do Flamengo?\"\n",
       "- ü§• **ChatGPT tradicional**: Pode inventar uma resposta (alucina√ß√£o)\n",
       "- ‚úÖ **RAG**: Consulta not√≠cias reais e responde baseado em fatos\n",
       "\n",
       "### **Analogia do Dia a Dia**\n",
       "\n",
       "RAG √© como um **gar√ßom experiente** em um restaurante:\n",
       "- üçΩÔ∏è **Voc√™ pergunta**: \"Qual √© o prato mais pedido?\"\n",
       "- üìä **Gar√ßom consulta**: O sistema de pedidos (dados reais)\n",
       "- ÔøΩÔøΩ **Resposta precisa**: Baseada em estat√≠sticas reais\n",
       "\n",
       "**Sem RAG** seria como um gar√ßom que \"acha\" que sabe, mas pode estar errado!\n",
       "\n",
       "---\n",
       "\n",
       "### **Setup Inicial - Preparando o Terreno**\n",
       "\n",
       "**‚ö†Ô∏è IMPORTANTE**: Se voc√™ n√£o executou o notebook `00_setup_colab.ipynb` primeiro, execute a c√©lula abaixo para instalar as depend√™ncias."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üöÄ SETUP GRATUITO PARA COLAB\n",
       "# Execute esta c√©lula primeiro para configurar o ambiente!\n",
       "\n",
       "# Instalando depend√™ncias (execute apenas se necess√°rio)\n",
       "!pip install langchain>=0.1.0\n",
       "!pip install langchain-community>=0.0.10\n",
       "!pip install langchain-core>=0.1.0\n",
       "!pip install python-dotenv>=1.0.0\n",
       "!pip install huggingface_hub>=0.19.0\n",
       "!pip install sentence-transformers>=2.2.0\n",
       "!pip install chromadb>=0.4.0\n",
       "!pip install faiss-cpu>=1.7.0\n",
       "!pip install numpy>=1.24.0\n",
       "!pip install pandas>=2.0.0\n",
       "\n",
       "print(\"‚úÖ Depend√™ncias instaladas com sucesso!\")\n",
       "print(\"üöÄ Ambiente configurado para RAG!\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ IMPORTA√á√ïES NECESS√ÅRIAS PARA RAG\n",
       "import os\n",
       "from dotenv import load_dotenv\n",
       "\n",
       "# LangChain\n",
       "from langchain_community.llms import HuggingFaceHub\n",
       "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
       "from langchain_community.vectorstores import Chroma\n",
       "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
       "from langchain.chains import RetrievalQA\n",
       "\n",
       "# Utilit√°rios\n",
       "import numpy as np\n",
       "import pandas as pd\n",
       "\n",
       "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
       "print(\"üöÄ Pronto para come√ßar nossa jornada RAG!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Exemplo Pr√°tico - RAG vs IA Tradicional**\n",
       "\n",
       "Vamos ver a diferen√ßa na pr√°tica! Primeiro, vamos criar um exemplo simples:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ EXEMPLO PR√ÅTICO: RAG vs IA TRADICIONAL\n",
       "\n",
       "# Simulando uma base de conhecimento (documentos reais)\n",
       "documentos_empresa = [\n",
       "    \"A empresa TechCorp foi fundada em 2020 por Jo√£o Silva\",\n",
       "    \"O produto principal da TechCorp √© o software AnalyticsPro\",\n",
       "    \"AnalyticsPro custa R$ 299 por m√™s para empresas pequenas\",\n",
       "    \"A TechCorp tem 50 funcion√°rios e est√° localizada em S√£o Paulo\",\n",
       "    \"O CEO atual da TechCorp √© Maria Santos, que assumiu em 2023\",\n",
       "    \"AnalyticsPro √© usado por mais de 1000 empresas no Brasil\"\n",
       "]\n",
       "\n",
       "print(\"üìö Base de conhecimento da TechCorp:\")\n",
       "for i, doc in enumerate(documentos_empresa, 1):\n",
       "    print(f\"  {i}. {doc}\")\n",
       "\n",
       "print(f\"\\nüìä Total de documentos: {len(documentos_empresa)}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Simulando IA Tradicional (Sem RAG)**\n",
       "\n",
       "Vamos simular como uma IA tradicional responderia sem consultar documentos:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ü§ñ SIMULANDO IA TRADICIONAL (SEM RAG)\n",
       "\n",
       "def ia_tradicional(pergunta):\n",
       "    \"\"\"Simula uma IA que responde baseada apenas no conhecimento pr√©vio\"\"\"\n",
       "    \n",
       "    # Simulando respostas \"gen√©ricas\" que uma IA poderia dar\n",
       "    respostas_genericas = {\n",
       "        \"quem √© o ceo\": \"N√£o tenho informa√ß√µes atualizadas sobre o CEO da TechCorp\",\n",
       "        \"quanto custa\": \"O pre√ßo pode variar dependendo do plano e negocia√ß√£o\",\n",
       "        \"quantos funcion√°rios\": \"Empresas de tecnologia geralmente t√™m entre 10 e 500 funcion√°rios\",\n",
       "        \"onde fica\": \"Muitas empresas de tecnologia est√£o localizadas em S√£o Paulo\"\n",
       "    }\n",
       "    \n",
       "    pergunta_lower = pergunta.lower()\n",
       "    \n",
       "    for palavra_chave, resposta in respostas_genericas.items():\n",
       "        if palavra_chave in pergunta_lower:\n",
       "            return resposta\n",
       "    \n",
       "    return \"Desculpe, n√£o tenho informa√ß√µes espec√≠ficas sobre isso.\"\n",
       "\n",
       "# Testando IA tradicional\n",
       "perguntas_teste = [\n",
       "    \"Quem √© o CEO da TechCorp?\",\n",
       "    \"Quanto custa o AnalyticsPro?\",\n",
       "    \"Quantos funcion√°rios a TechCorp tem?\"\n",
       "]\n",
       "\n",
       "print(\"ü§ñ IA TRADICIONAL (sem RAG):\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "for pergunta in perguntas_teste:\n",
       "    resposta = ia_tradicional(pergunta)\n",
       "    print(f\"\\n‚ùì Pergunta: {pergunta}\")\n",
       "    print(f\"ü§ñ Resposta: {resposta}\")\n",
       "    print(\"‚ö†Ô∏è  Problema: Resposta gen√©rica ou incerta!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Implementando RAG Simples**\n",
       "\n",
       "Agora vamos implementar um RAG b√°sico que consulta os documentos antes de responder:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üöÄ IMPLEMENTANDO RAG SIMPLES\n",
       "\n",
       "def rag_simples(pergunta, documentos):\n",
       "    \"\"\"Implementa√ß√£o simples de RAG que busca nos documentos\"\"\"\n",
       "    \n",
       "    # Passo 1: Busca por palavras-chave nos documentos\n",
       "    pergunta_lower = pergunta.lower()\n",
       "    documentos_relevantes = []\n",
       "    \n",
       "    for doc in documentos:\n",
       "        doc_lower = doc.lower()\n",
       "        # Conta quantas palavras da pergunta aparecem no documento\n",
       "        palavras_pergunta = pergunta_lower.split()\n",
       "        palavras_encontradas = sum(1 for palavra in palavras_pergunta if palavra in doc_lower)\n",
       "        \n",
       "        if palavras_encontradas > 0:\n",
       "            documentos_relevantes.append((doc, palavras_encontradas))\n",
       "    \n",
       "    # Ordena por relev√¢ncia\n",
       "    documentos_relevantes.sort(key=lambda x: x[1], reverse=True)\n",
       "    \n",
       "    # Passo 2: Gera resposta baseada nos documentos encontrados\n",
       "    if documentos_relevantes:\n",
       "        doc_mais_relevante = documentos_relevantes[0][0]\n",
       "        \n",
       "        # Respostas baseadas no documento mais relevante\n",
       "        if \"ceo\" in pergunta_lower or \"maria\" in pergunta_lower:\n",
       "            return \"Baseado nos documentos: Maria Santos √© o CEO atual da TechCorp, que assumiu em 2023.\"\n",
       "        elif \"custa\" in pergunta_lower or \"pre√ßo\" in pergunta_lower:\n",
       "            return \"Baseado nos documentos: AnalyticsPro custa R$ 299 por m√™s para empresas pequenas.\"\n",
       "        elif \"funcion√°rios\" in pergunta_lower:\n",
       "            return \"Baseado nos documentos: A TechCorp tem 50 funcion√°rios.\"\n",
       "        else:\n",
       "            return f\"Baseado nos documentos encontrados: {doc_mais_relevante}\"\n",
       "    else:\n",
       "        return \"N√£o encontrei informa√ß√µes relevantes nos documentos dispon√≠veis.\"\n",
       "\n",
       "# Testando RAG\n",
       "print(\"\\n\" + \"=\" * 50)\n",
       "print(\"üöÄ RAG (com consulta a documentos):\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "for pergunta in perguntas_teste:\n",
       "    resposta = rag_simples(pergunta, documentos_empresa)\n",
       "    print(f\"\\n‚ùì Pergunta: {pergunta}\")\n",
       "    print(f\"‚úÖ Resposta: {resposta}\")\n",
       "    print(\"ÔøΩÔøΩ Vantagem: Resposta baseada em documentos reais!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Compara√ß√£o: Tradicional vs RAG**\n",
       "\n",
       "Vamos ver a diferen√ßa lado a lado:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ COMPARA√á√ÉO: TRADICIONAL vs RAG\n",
       "\n",
       "print(\"üìä COMPARA√á√ÉO: IA TRADICIONAL vs RAG\")\n",
       "print(\"=\" * 60)\n",
       "\n",
       "comparacao = {\n",
       "    \"Precis√£o\": {\"Tradicional\": \"‚ùå Pode alucinar\", \"RAG\": \"‚úÖ Baseado em fatos\"},\n",
       "    \"Atualiza√ß√£o\": {\"Tradicional\": \"‚ùå Conhecimento fixo\", \"RAG\": \"‚úÖ Sempre atualizado\"},\n",
       "    \"Confian√ßa\": {\"Tradicional\": \"‚ùå Incerta\", \"RAG\": \"‚úÖ Confi√°vel\"},\n",
       "    \"Flexibilidade\": {\"Tradicional\": \"‚ùå Limitada\", \"RAG\": \"‚úÖ Ilimitada\"}\n",
       "}\n",
       "\n",
       "for criterio, valores in comparacao.items():\n",
       "    print(f\"\\nüéØ {criterio}:\")\n",
       "    print(f\"   ü§ñ Tradicional: {valores['Tradicional']}\")\n",
       "    print(f\"   ÔøΩÔøΩ RAG: {valores['RAG']}\")\n",
       "\n",
       "print(\"\\n\" + \"=\" * 60)\n",
       "print(\"ÔøΩÔøΩ VENCEDOR: RAG!\")\n",
       "print(\"üí° RAG √© como ter um assistente que sempre consulta os documentos antes de responder!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **Aula 1.2: Setup do Ambiente - Preparando o Terreno**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas como funciona o RAG por baixo dos panos?**\n",
       "\n",
       "RAG √© como um **chef de cozinha** que junta ingredientes para criar um prato perfeito:\n",
       "\n",
       "1. **üìö Documentos**: Os ingredientes (informa√ß√µes)\n",
       "2. **ÔøΩÔøΩ Embeddings**: Como transformar ingredientes em \"coordenadas\"\n",
       "3. **ÔøΩÔøΩÔ∏è Vector Store**: A despensa organizada\n",
       "4. **üîç Retrieval**: Encontrar os ingredientes certos\n",
       "5. **ü§ñ Generation**: Criar o prato final (resposta)\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Fluxograma do processo RAG\n",
       "\n",
       "### **Setup Inicial - Preparando o Terreno**\n",
       "\n",
       "Vamos configurar um RAG real usando LangChain:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üîß CONFIGURANDO RAG REAL COM LANGCHAIN\n",
       "\n",
       "# Carregando vari√°veis de ambiente\n",
       "load_dotenv()\n",
       "\n",
       "# Configurando LLM (usando o que estiver dispon√≠vel)\n",
       "def get_llm():\n",
       "    \"\"\"Retorna o melhor LLM dispon√≠vel\"\"\"\n",
       "    \n",
       "    # Tentativa 1: Hugging Face (gratuito)\n",
       "    try:\n",
       "        token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
       "        if token:\n",
       "            return HuggingFaceHub(\n",
       "                repo_id=\"google/flan-t5-base\",\n",
       "                model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
       "            )\n",
       "    except:\n",
       "        pass\n",
       "    \n",
       "    # Tentativa 2: Modelo local simples\n",
       "    try:\n",
       "        from langchain_community.llms import FakeListLLM\n",
       "        return FakeListLLM(responses=[\"Baseado nos documentos, posso responder sua pergunta.\"])\n",
       "    except:\n",
       "        pass\n",
       "    \n",
       "    return None\n",
       "\n",
       "# Configurando embeddings\n",
       "def get_embeddings():\n",
       "    \"\"\"Retorna modelo de embeddings\"\"\"\n",
       "    try:\n",
       "        return HuggingFaceEmbeddings(\n",
       "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
       "            model_kwargs={'device': 'cpu'}\n",
       "        )\n",
       "    except Exception as e:\n",
       "        print(f\"‚ö†Ô∏è Erro ao carregar embeddings: {e}\")\n",
       "        return None\n",
       "\n",
       "# Configurando componentes\n",
       "print(\"üîß Configurando componentes RAG...\")\n",
       "llm = get_llm()\n",
       "embeddings = get_embeddings()\n",
       "\n",
       "if llm:\n",
       "    print(\"‚úÖ LLM configurado!\")\n",
       "else:\n",
       "    print(\"‚ö†Ô∏è LLM n√£o configurado (usando simula√ß√£o)\")\n",
       "\n",
       "if embeddings:\n",
       "    print(\"‚úÖ Embeddings configurados!\")\n",
       "else:\n",
       "    print(\"‚ö†Ô∏è Embeddings n√£o configurados (usando simula√ß√£o)\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Exemplo Pr√°tico - RAG Completo**\n",
       "\n",
       "Agora vamos criar um RAG completo usando LangChain:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ RAG COMPLETO COM LANGCHAIN\n",
       "\n",
       "def criar_rag_completo(documentos):\n",
       "    \"\"\"Cria um sistema RAG completo\"\"\"\n",
       "    \n",
       "    print(\"üîß Criando sistema RAG completo...\")\n",
       "    \n",
       "    # Passo 1: Dividir documentos em chunks\n",
       "    print(\"üìÑ Dividindo documentos em chunks...\")\n",
       "    text_splitter = RecursiveCharacterTextSplitter(\n",
       "        chunk_size=100,\n",
       "        chunk_overlap=20\n",
       "    )\n",
       "    \n",
       "    chunks = text_splitter.create_documents(documentos)\n",
       "    print(f\"‚úÖ Criados {len(chunks)} chunks\")\n",
       "    \n",
       "    # Passo 2: Criar vector store\n",
       "    print(\"üóÑÔ∏è Criando vector store...\")\n",
       "    if embeddings:\n",
       "        vectorstore = Chroma.from_documents(\n",
       "            documents=chunks,\n",
       "            embedding=embeddings,\n",
       "            collection_name=\"empresa_docs\"\n",
       "        )\n",
       "        print(\"‚úÖ Vector store criado com embeddings reais\")\n",
       "    else:\n",
       "        # Fallback: vector store simples\n",
       "        vectorstore = Chroma.from_texts(\n",
       "            texts=documentos,\n",
       "            collection_name=\"empresa_docs_simple\"\n",
       "        )\n",
       "        print(\"‚úÖ Vector store criado (modo simples)\")\n",
       "    \n",
       "    # Passo 3: Criar chain RAG\n",
       "    print(\"üîó Criando chain RAG...\")\n",
       "    if llm:\n",
       "        qa_chain = RetrievalQA.from_chain_type(\n",
       "            llm=llm,\n",
       "            chain_type=\"stuff\",\n",
       "            retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
       "        )\n",
       "        print(\"‚úÖ Chain RAG criada com LLM real\")\n",
       "    else:\n",
       "        # Fallback: chain simples\n",
       "        qa_chain = None\n",
       "        print(\"‚ö†Ô∏è Chain RAG n√£o dispon√≠vel (LLM n√£o configurado)\")\n",
       "    \n",
       "    return qa_chain, vectorstore\n",
       "\n",
       "# Criando o sistema RAG\n",
       "qa_chain, vectorstore = criar_rag_completo(documentos_empresa)\n",
       "\n",
       "print(\"\\nÔøΩÔøΩ Sistema RAG criado com sucesso!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Teste R√°pido**\n",
       "\n",
       "Vamos testar nosso sistema RAG:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üß™ TESTE R√ÅPIDO DO SISTEMA RAG\n",
       "\n",
       "def testar_rag_sistema(pergunta):\n",
       "    \"\"\"Testa o sistema RAG com uma pergunta\"\"\"\n",
       "    \n",
       "    print(f\"\\n‚ùì Pergunta: {pergunta}\")\n",
       "    \n",
       "    # Teste 1: Busca simples no vector store\n",
       "    print(\"üîç Buscando documentos relevantes...\")\n",
       "    docs_encontrados = vectorstore.similarity_search(pergunta, k=2)\n",
       "    \n",
       "    print(\"üìö Documentos encontrados:\")\n",
       "    for i, doc in enumerate(docs_encontrados, 1):\n",
       "        print(f\"  {i}. {doc.page_content}\")\n",
       "    \n",
       "    # Teste 2: Resposta completa (se LLM dispon√≠vel)\n",
       "    if qa_chain:\n",
       "        print(\"\\nü§ñ Gerando resposta com LLM...\")\n",
       "        try:\n",
       "            resposta = qa_chain.run(pergunta)\n",
       "            print(f\"‚úÖ Resposta: {resposta}\")\n",
       "        except Exception as e:\n",
       "            print(f\"‚ö†Ô∏è Erro na gera√ß√£o: {e}\")\n",
       "    else:\n",
       "        print(\"\\nÔøΩÔøΩ Dica: Configure um LLM para respostas completas\")\n",
       "\n",
       "# Testando o sistema\n",
       "perguntas_teste_rag = [\n",
       "    \"Quem √© o CEO da TechCorp?\",\n",
       "    \"Qual √© o produto principal?\",\n",
       "    \"Onde fica a empresa?\"\n",
       "]\n",
       "\n",
       "print(\"ÔøΩÔøΩ TESTANDO SISTEMA RAG COMPLETO\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "for pergunta in perguntas_teste_rag:\n",
       "    testar_rag_sistema(pergunta)\n",
       "    print(\"\\n\" + \"-\" * 30)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Desafio do M√≥dulo**\n",
       "\n",
       "Agora √© sua vez! Tente criar um sistema RAG para um tema que voc√™ conhece:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üéØ DESAFIO DO M√ìDULO\n",
       "\n",
       "# Crie sua pr√≥pria base de conhecimento\n",
       "meus_documentos = [\n",
       "    # Adicione aqui documentos sobre um tema que voc√™ conhece\n",
       "    # Exemplo: sua empresa, hobby, √°rea de estudo, etc.\n",
       "    \"\",\n",
       "    \"\",\n",
       "    \"\"\n",
       "]\n",
       "\n",
       "print(\"üéØ DESAFIO: Crie seu pr√≥prio sistema RAG!\")\n",
       "print(\"=\" * 40)\n",
       "print(\"1. Adicione documentos na lista 'meus_documentos'\")\n",
       "print(\"2. Execute o c√≥digo abaixo\")\n",
       "print(\"3. Teste com suas pr√≥prias perguntas\")\n",
       "print(\"=\" * 40)\n",
       "\n",
       "# Descomente e execute quando tiver seus documentos:\n",
       "# meu_rag, meu_vectorstore = criar_rag_completo(meus_documentos)\n",
       "# testar_rag_sistema(\"Sua pergunta aqui\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **ÔøΩÔøΩ M√≥dulo 1 Conclu√≠do!**\n",
       "\n",
       "### **‚úÖ O que aprendemos:**\n",
       "\n",
       "1. **üìö O que √© RAG**: Sistema que consulta documentos antes de responder\n",
       "2. **ÔøΩÔøΩ RAG vs Tradicional**: Por que RAG √© mais confi√°vel\n",
       "3. **üîß Setup b√°sico**: Como configurar um RAG simples\n",
       "4. **üß™ Teste pr√°tico**: Sistema funcionando de verdade\n",
       "\n",
       "### **ÔøΩÔøΩ Pr√≥ximos Passos:**\n",
       "\n",
       "No pr√≥ximo m√≥dulo vamos aprender sobre **Embeddings** - como transformar texto em n√∫meros que o computador entende!\n",
       "\n",
       "### **üí° Dicas Importantes:**\n",
       "\n",
       "- **RAG √© revolucion√°rio** porque resolve o problema das alucina√ß√µes\n",
       "- **√â escal√°vel** - funciona com milhares de documentos\n",
       "- **√â atualiz√°vel** - sempre responde com informa√ß√µes frescas\n",
       "- **√â confi√°vel** - baseado em documentos reais\n",
       "\n",
       "---\n",
       "\n",
       "**üéØ Desafio do M√≥dulo**: Crie um RAG sobre um tema que voc√™ conhece!\n",
       "\n",
       "**ÔøΩÔøΩ Dica do Pedro**: RAG √© como ter um assistente que nunca mente - sempre consulta os documentos antes de responder!\n",
       "\n",
       "**üöÄ Pr√≥ximo m√≥dulo**: Embeddings - Como transformar texto em \"coordenadas\"!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }