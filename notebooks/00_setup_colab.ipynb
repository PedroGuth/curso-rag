{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# üöÄ **Setup Gratuito para RAG no Google Colab**\n",
       "\n",
       "> *Porque RAG n√£o precisa ser caro para ser poderoso*\n",
       "\n",
       "---\n",
       "\n",
       "## **üéØ O que vamos fazer aqui?**\n",
       "\n",
       "Vamos configurar um ambiente **100% gratuito** para aprender RAG. Sem OpenAI pago, sem servidores caros - s√≥ o que funciona de gra√ßa!\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Um setup de laborat√≥rio com ferramentas gratuitas\n",
       "\n",
       "### **T√°, mas por que RAG precisa de setup especial?**\n",
       "\n",
       "RAG √© tipo montar uma **biblioteca digital inteligente**. Voc√™ precisa de:\n",
       "- üìö **Ferramentas para ler documentos** (PDFs, websites, etc.)\n",
       "- üß† **Modelos para entender texto** (embeddings)\n",
       "- üóÑÔ∏è **Local para guardar informa√ß√µes** (vector stores)\n",
       "- ü§ñ **IA para gerar respostas** (LLMs)\n",
       "\n",
       "**Por que RAG √© importante?**\n",
       "\n",
       "Imagine ter um assistente que **nunca mente** e sempre responde baseado em documentos reais. √â isso que o RAG faz!\n",
       "\n",
       "---\n",
       "\n",
       "## **üÜì Nossa Estrat√©gia Gratuita**\n",
       "\n",
       "### **Op√ß√µes de LLM (em ordem de prefer√™ncia):**\n",
       "1. **Hugging Face** (30.000 requisi√ß√µes/m√™s gr√°tis)\n",
       "2. **OpenAI** (se voc√™ tiver API key)\n",
       "\n",
       "### **Vector Stores Gratuitos:**\n",
       "1. **Chroma** (local, super r√°pido)\n",
       "2. **FAISS** (local, para datasets grandes)\n",
       "\n",
       "### **Embeddings Gratuitos:**\n",
       "1. **Sentence Transformers** (local, qualidade excelente)\n",
       "2. **Hugging Face Embeddings** (online, 30k/m√™s gr√°tis)\n",
       "\n",
       "---"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **üîß Passo 1: Instalando as Depend√™ncias**\n",
       "\n",
       "Primeiro, vamos instalar todas as bibliotecas que precisamos. √â como montar uma caixa de ferramentas!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üõ†Ô∏è INSTALANDO AS FERRAMENTAS DO RAG\n",
       "# Execute esta c√©lula primeiro!\n",
       "\n",
       "!pip install langchain>=0.1.0\n",
       "!pip install langchain-community>=0.0.10\n",
       "!pip install langchain-core>=0.1.0\n",
       "!pip install langchain-openai>=0.0.5\n",
       "!pip install python-dotenv>=1.0.0\n",
       "\n",
       "# Embeddings e Vector Stores\n",
       "!pip install sentence-transformers>=2.2.0\n",
       "!pip install chromadb>=0.4.0\n",
       "!pip install faiss-cpu>=1.7.0\n",
       "\n",
       "# Document Loaders\n",
       "!pip install pypdf>=3.15.0\n",
       "!pip install python-docx>=0.8.11\n",
       "!pip install beautifulsoup4>=4.12.0\n",
       "!pip install requests>=2.31.0\n",
       "!pip install youtube-transcript-api>=0.6.0\n",
       "!pip install pandas>=2.0.0\n",
       "\n",
       "# LLMs\n",
       "!pip install huggingface_hub>=0.19.0\n",
       "!pip install openai>=1.0.0\n",
       "!pip install transformers>=4.30.0\n",
       "!pip install torch>=2.0.0\n",
       "\n",
       "# Deploy e Interface\n",
       "!pip install streamlit>=1.28.0\n",
       "!pip install gradio>=4.0.0\n",
       "!pip install fastapi>=0.100.0\n",
       "!pip install uvicorn>=0.23.0\n",
       "\n",
       "# Utilit√°rios\n",
       "!pip install numpy>=1.24.0\n",
       "!pip install scikit-learn>=1.3.0\n",
       "!pip install tiktoken>=0.5.0\n",
       "\n",
       "print(\"‚úÖ Todas as depend√™ncias instaladas com sucesso!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **üîë Passo 2: Configurando as Chaves de API**\n",
       "\n",
       "Agora vamos configurar as chaves de API. **N√£o se preocupe** - vamos usar op√ß√µes gratuitas!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üîë CONFIGURANDO AS CHAVES DE API\n",
       "import os\n",
       "from dotenv import load_dotenv\n",
       "\n",
       "# Carregando vari√°veis de ambiente (se existirem)\n",
       "load_dotenv()\n",
       "\n",
       "print(\"üîß Configurando ambiente RAG...\")\n",
       "print(\"üìã Verificando chaves de API dispon√≠veis...\")\n",
       "\n",
       "# Verificando OpenAI (opcional)\n",
       "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
       "if openai_key:\n",
       "    print(\"‚úÖ OpenAI API Key encontrada!\")\n",
       "else:\n",
       "    print(\"‚ÑπÔ∏è OpenAI API Key n√£o encontrada (vamos usar Hugging Face)\")\n",
       "\n",
       "# Verificando Hugging Face (recomendado)\n",
       "huggingface_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
       "if huggingface_token:\n",
       "    print(\"‚úÖ Hugging Face Token encontrado!\")\n",
       "else:\n",
       "    print(\"‚ÑπÔ∏è Hugging Face Token n√£o encontrado (vamos usar modelos locais)\")\n",
       "\n",
       "print(\"\\nüéØ Estrat√©gia: Vamos usar modelos locais gratuitos!\")\n",
       "print(\"üí° Dica: Se quiser melhor qualidade, configure as chaves de API acima\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **ü§ñ Passo 3: Configurando o LLM Gratuito**\n",
       "\n",
       "Agora vamos configurar o LLM que vai gerar as respostas. Vamos usar o **melhor dispon√≠vel**!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ü§ñ CONFIGURANDO O LLM GRATUITO\n",
       "def get_llm_colab():\n",
       "    \"\"\"Retorna o melhor LLM dispon√≠vel no Colab\"\"\"\n",
       "    \n",
       "    # Tentativa 1: OpenAI (se tiver chave)\n",
       "    try:\n",
       "        from langchain_openai import ChatOpenAI\n",
       "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
       "        if api_key:\n",
       "            print(\"üöÄ Usando OpenAI GPT-3.5-turbo\")\n",
       "            return ChatOpenAI(\n",
       "                model=\"gpt-3.5-turbo\",\n",
       "                temperature=0.7,\n",
       "                api_key=api_key\n",
       "            )\n",
       "    except Exception as e:\n",
       "        print(f\"‚ÑπÔ∏è OpenAI n√£o dispon√≠vel: {e}\")\n",
       "    \n",
       "    # Tentativa 2: Hugging Face (gratuito)\n",
       "    try:\n",
       "        from langchain_community.llms import HuggingFaceHub\n",
       "        token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
       "        if token:\n",
       "            print(\"üöÄ Usando Hugging Face Hub (gratuito)\")\n",
       "            return HuggingFaceHub(\n",
       "                repo_id=\"google/flan-t5-base\",\n",
       "                model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
       "            )\n",
       "    except Exception as e:\n",
       "        print(f\"‚ÑπÔ∏è Hugging Face Hub n√£o dispon√≠vel: {e}\")\n",
       "    \n",
       "    # Tentativa 3: Modelo local (sempre funciona)\n",
       "    try:\n",
       "        from langchain_community.llms import HuggingFacePipeline\n",
       "        from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
       "        \n",
       "        print(\"üöÄ Usando modelo local (gratuito e r√°pido)\")\n",
       "        \n",
       "        # Modelo pequeno e r√°pido para demonstra√ß√£o\n",
       "        model_name = \"microsoft/DialoGPT-small\"\n",
       "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
       "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
       "        \n",
       "        # Criando o pipeline\n",
       "        pipe = pipeline(\n",
       "            \"text-generation\",\n",
       "            model=model,\n",
       "            tokenizer=tokenizer,\n",
       "            max_length=100,\n",
       "            temperature=0.7,\n",
       "            do_sample=True\n",
       "        )\n",
       "        \n",
       "        return HuggingFacePipeline(pipeline=pipe)\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"‚ùå Erro ao carregar modelo local: {e}\")\n",
       "        return None\n",
       "\n",
       "# Configurando o LLM\n",
       "print(\"üîß Configurando LLM...\")\n",
       "llm = get_llm_colab()\n",
       "\n",
       "if llm:\n",
       "    print(\"‚úÖ LLM configurado com sucesso!\")\n",
       "    \n",
       "    # Teste r√°pido\n",
       "    try:\n",
       "        print(\"\\nüß™ Teste r√°pido do LLM:\")\n",
       "        response = llm.invoke(\"Diga 'Ol√°, RAG est√° funcionando!'\")\n",
       "        print(f\"Resposta: {response}\")\n",
       "    except Exception as e:\n",
       "        print(f\"‚ö†Ô∏è Teste falhou (normal para alguns modelos): {e}\")\n",
       "else:\n",
       "    print(\"‚ùå N√£o foi poss√≠vel configurar o LLM\")\n",
       "    print(\"üí° Configure uma chave de API ou use o modelo local\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **üß† Passo 4: Configurando Embeddings Gratuitos**\n",
       "\n",
       "Agora vamos configurar os embeddings - que s√£o como \"tradutores\" que transformam texto em n√∫meros que o computador entende."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üß† CONFIGURANDO EMBEDDINGS GRATUITOS\n",
       "def get_embeddings_colab():\n",
       "    \"\"\"Retorna o melhor modelo de embeddings dispon√≠vel\"\"\"\n",
       "    \n",
       "    # Op√ß√£o 1: Sentence Transformers (local, excelente qualidade)\n",
       "    try:\n",
       "        from langchain_community.embeddings import HuggingFaceEmbeddings\n",
       "        \n",
       "        print(\"üöÄ Usando Sentence Transformers (local e gratuito)\")\n",
       "        \n",
       "        # Modelo pequeno e r√°pido para demonstra√ß√£o\n",
       "        embeddings = HuggingFaceEmbeddings(\n",
       "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
       "            model_kwargs={'device': 'cpu'}\n",
       "        )\n",
       "        \n",
       "        return embeddings\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"‚ùå Erro ao carregar Sentence Transformers: {e}\")\n",
       "        return None\n",
       "\n",
       "# Configurando embeddings\n",
       "print(\"üîß Configurando embeddings...\")\n",
       "embeddings = get_embeddings_colab()\n",
       "\n",
       "if embeddings:\n",
       "    print(\"‚úÖ Embeddings configurados com sucesso!\")\n",
       "    \n",
       "    # Teste r√°pido\n",
       "    try:\n",
       "        print(\"\\nüß™ Teste r√°pido dos embeddings:\")\n",
       "        texts = [\"Ol√° mundo\", \"Hello world\", \"Gato preto\"]\n",
       "        vectors = embeddings.embed_documents(texts)\n",
       "        print(f\"‚úÖ Gerados {len(vectors)} embeddings de {len(texts)} textos\")\n",
       "        print(f\"üìè Dimens√£o de cada embedding: {len(vectors[0])}\")\n",
       "    except Exception as e:\n",
       "        print(f\"‚ö†Ô∏è Teste falhou: {e}\")\n",
       "else:\n",
       "    print(\"‚ùå N√£o foi poss√≠vel configurar embeddings\")\n",
       "    print(\"üí° Verifique se as depend√™ncias foram instaladas corretamente\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **üóÑÔ∏è Passo 5: Configurando Vector Store Gratuito**\n",
       "\n",
       "Agora vamos configurar onde vamos guardar as informa√ß√µes. Vamos usar o **Chroma** - que √© local e super r√°pido!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üóÑÔ∏è CONFIGURANDO VECTOR STORE GRATUITO\n",
       "def setup_vector_store():\n",
       "    \"\"\"Configura o vector store para armazenar embeddings\"\"\"\n",
       "    \n",
       "    try:\n",
       "        from langchain_community.vectorstores import Chroma\n",
       "        \n",
       "        print(\"üöÄ Configurando Chroma (vector store local e gratuito)\")\n",
       "        \n",
       "        # Criando um vector store tempor√°rio para teste\n",
       "        texts = [\n",
       "            \"RAG √© uma t√©cnica de IA que combina busca e gera√ß√£o\",\n",
       "            \"Embeddings s√£o representa√ß√µes num√©ricas de texto\",\n",
       "            \"Vector stores armazenam embeddings para busca r√°pida\",\n",
       "            \"LangChain √© um framework para construir aplica√ß√µes de IA\"\n",
       "        ]\n",
       "        \n",
       "        # Criando o vector store\n",
       "        vectorstore = Chroma.from_texts(\n",
       "            texts=texts,\n",
       "            embedding=embeddings,\n",
       "            collection_name=\"test_collection\"\n",
       "        )\n",
       "        \n",
       "        print(\"‚úÖ Vector store configurado com sucesso!\")\n",
       "        \n",
       "        # Teste de busca\n",
       "        print(\"\\nüß™ Teste de busca no vector store:\")\n",
       "        query = \"O que √© RAG?\"\n",
       "        results = vectorstore.similarity_search(query, k=2)\n",
       "        \n",
       "        print(f\"üîç Busca por: '{query}'\")\n",
       "        for i, doc in enumerate(results, 1):\n",
       "            print(f\"  {i}. {doc.page_content}\")\n",
       "        \n",
       "        return vectorstore\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"‚ùå Erro ao configurar vector store: {e}\")\n",
       "        return None\n",
       "\n",
       "# Configurando vector store\n",
       "print(\"üîß Configurando vector store...\")\n",
       "vectorstore = setup_vector_store()\n",
       "\n",
       "if vectorstore:\n",
       "    print(\"\\nüéâ Vector store funcionando perfeitamente!\")\n",
       "else:\n",
       "    print(\"‚ùå N√£o foi poss√≠vel configurar vector store\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **üéØ Passo 6: Teste Completo do Sistema**\n",
       "\n",
       "Agora vamos fazer um teste completo para ver se tudo est√° funcionando!"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üéØ TESTE COMPLETO DO SISTEMA RAG\n",
       "def test_rag_system():\n",
       "    \"\"\"Testa se o sistema RAG est√° funcionando\"\"\"\n",
       "    \n",
       "    print(\"üß™ Iniciando teste completo do sistema RAG...\")\n",
       "    \n",
       "    # Verificando componentes\n",
       "    checks = {\n",
       "        \"LLM\": llm is not None,\n",
       "        \"Embeddings\": embeddings is not None,\n",
       "        \"Vector Store\": vectorstore is not None\n",
       "    }\n",
       "    \n",
       "    print(\"\\nüìã Status dos componentes:\")\n",
       "    for component, status in checks.items():\n",
       "        status_icon = \"‚úÖ\" if status else \"‚ùå\"\n",
       "        print(f\"  {status_icon} {component}\")\n",
       "    \n",
       "    # Se tudo estiver OK, fazer teste de RAG simples\n",
       "    if all(checks.values()):\n",
       "        print(\"\\nüöÄ Todos os componentes OK! Fazendo teste de RAG...\")\n",
       "        \n",
       "        try:\n",
       "            # Buscando documentos relevantes\n",
       "            query = \"Explique o que √© RAG\"\n",
       "            docs = vectorstore.similarity_search(query, k=2)\n",
       "            \n",
       "            # Criando contexto\n",
       "            context = \"\\n\".join([doc.page_content for doc in docs])\n",
       "            \n",
       "            # Gerando resposta\n",
       "            prompt = f\"\"\"\n",
       "            Com base no contexto abaixo, responda √† pergunta:\n",
       "            \n",
       "            Contexto:\n",
       "            {context}\n",
       "            \n",
       "            Pergunta: {query}\n",
       "            \n",
       "            Resposta:\n",
       "            \"\"\"\n",
       "            \n",
       "            response = llm.invoke(prompt)\n",
       "            \n",
       "            print(f\"\\nüîç Pergunta: {query}\")\n",
       "            print(f\"üìö Contexto encontrado: {len(docs)} documentos\")\n",
       "            print(f\"ü§ñ Resposta: {response}\")\n",
       "            \n",
       "            print(\"\\nüéâ SISTEMA RAG FUNCIONANDO PERFEITAMENTE!\")\n",
       "            \n",
       "        except Exception as e:\n",
       "            print(f\"‚ö†Ô∏è Teste de RAG falhou: {e}\")\n",
       "            print(\"üí° Isso √© normal se o LLM n√£o estiver configurado perfeitamente\")\n",
       "    else:\n",
       "        print(\"\\n‚ùå Alguns componentes n√£o est√£o funcionando\")\n",
       "        print(\"üí° Verifique as configura√ß√µes acima\")\n",
       "\n",
       "# Executando teste\n",
       "test_rag_system()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **üéâ Setup Conclu√≠do!**\n",
       "\n",
       "### **‚úÖ O que conseguimos configurar:**\n",
       "\n",
       "1. **üì¶ Depend√™ncias**: Todas as bibliotecas necess√°rias\n",
       "2. **üîë APIs**: Configura√ß√£o para op√ß√µes gratuitas\n",
       "3. **ü§ñ LLM**: Modelo de linguagem para gerar respostas\n",
       "4. **üß† Embeddings**: Transforma√ß√£o de texto em n√∫meros\n",
       "5. **üóÑÔ∏è Vector Store**: Armazenamento inteligente de informa√ß√µes\n",
       "6. **üß™ Teste**: Sistema RAG funcionando\n",
       "\n",
       "### **üöÄ Pr√≥ximos Passos:**\n",
       "\n",
       "Agora voc√™ pode:\n",
       "1. **Ir para o M√≥dulo 1**: Introdu√ß√£o ao RAG\n",
       "2. **Aprender embeddings**: Como transformar texto em n√∫meros\n",
       "3. **Explorar vector stores**: Onde guardar informa√ß√µes\n",
       "4. **Construir projetos**: Sistemas RAG completos\n",
       "\n",
       "### **üí° Dicas Importantes:**\n",
       "\n",
       "- **Este setup √© 100% gratuito** - n√£o precisa de dinheiro!\n",
       "- **Funciona offline** - n√£o depende de APIs externas\n",
       "- **√â escal√°vel** - pode crescer conforme necess√°rio\n",
       "- **√â profissional** - usado em empresas reais\n",
       "\n",
       "---\n",
       "\n",
       "**üéØ Desafio do Setup**: Tente fazer uma pergunta diferente no teste de RAG acima!\n",
       "\n",
       "**üí° Dica do Pedro**: Se algo n√£o funcionar, n√£o se preocupe! RAG √© como cozinhar - √†s vezes precisa ajustar o tempero.\n",
       "\n",
       "**üöÄ Pr√≥ximo m√≥dulo**: Introdu√ß√£o ao RAG - O que √© e por que √© revolucion√°rio!"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   } 