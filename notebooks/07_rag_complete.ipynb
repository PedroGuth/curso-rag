{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# üîç **M√≥dulo 7: RAG - Retrieval Augmented Generation**\n",
       "\n",
       "## **Aula 7.1: RAG B√°sico - IA que Sabe o que N√£o Sabe**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas o que √© RAG?**\n",
       "\n",
       "Imagina que voc√™ √© um **estudante muito inteligente** que est√° fazendo uma prova:\n",
       "\n",
       "**Sem RAG**: Voc√™ s√≥ pode usar o que j√° sabe de cabe√ßa. Se n√£o souber, chuta ou inventa.\n",
       "**Com RAG**: Voc√™ pode **consultar livros** antes de responder, garantindo que sua resposta seja precisa!\n",
       "\n",
       "**RAG** = **Retrieval Augmented Generation**\n",
       "- **Retrieval**: Busca informa√ß√µes relevantes\n",
       "- **Augmented**: Aumenta o conhecimento da IA\n",
       "- **Generation**: Gera respostas baseadas nas informa√ß√µes encontradas\n",
       "\n",
       "### **Por que RAG √© Revolucion√°rio?**\n",
       "\n",
       "**Problema tradicional**: IA s√≥ sabe o que foi treinada, n√£o pode acessar informa√ß√µes atualizadas\n",
       "**Solu√ß√£o RAG**: IA busca informa√ß√µes relevantes e gera respostas precisas\n",
       "\n",
       "√â como a diferen√ßa entre **um professor que s√≥ sabe o que estudou h√° 10 anos** vs **um professor que sempre consulta os livros mais recentes**! üìö\n",
       "\n",
       "---\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Um diagrama mostrando IA consultando documentos antes de responder"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Setup Inicial - Preparando o Terreno**"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Importando bibliotecas para RAG\n",
       "import os\n",
       "from dotenv import load_dotenv\n",
       "from langchain_openai import ChatOpenAI\n",
       "from langchain.embeddings import OpenAIEmbeddings\n",
       "from langchain.vectorstores import Chroma\n",
       "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
       "from langchain.chains import RetrievalQA\n",
       "from langchain.document_loaders import TextLoader\n",
       "\n",
       "# Carregando vari√°veis\n",
       "load_dotenv()\n",
       "\n",
       "# Modelo\n",
       "llm = ChatOpenAI(\n",
       "    model=\"gpt-3.5-turbo\",\n",
       "    temperature=0.7,\n",
       "    api_key=os.getenv('OPENAI_API_KEY')  # Configure sua API key no Colab\n",
       ")\n",
       "\n",
       "print(\"üîç Setup completo para RAG!\")\n",
       "print(f\"ÔøΩÔøΩ Modelo: {llm.model_name}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Criando Base de Conhecimento**\n",
       "\n",
       "Vamos criar uma base de conhecimento sobre tecnologia para testar o RAG:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Criando base de conhecimento sobre tecnologia\n",
       "# Como criar uma biblioteca especializada\n",
       "\n",
       "conhecimento_tech = \"\"\"\n",
       "PYTHON - LINGUAGEM DE PROGRAMA√á√ÉO\n",
       "\n",
       "Python √© uma linguagem de programa√ß√£o de alto n√≠vel, interpretada e orientada a objetos.\n",
       "Foi criada por Guido van Rossum e lan√ßada em 1991. Python √© conhecida por sua sintaxe\n",
       "simples e leg√≠vel, sendo ideal para iniciantes em programa√ß√£o.\n",
       "\n",
       "CARACTER√çSTICAS PRINCIPAIS:\n",
       "- Sintaxe clara e leg√≠vel\n",
       "- Grande biblioteca padr√£o\n",
       "- Suporte a m√∫ltiplos paradigmas\n",
       "- Comunidade ativa e extensa\n",
       "- Ideal para IA e ci√™ncia de dados\n",
       "\n",
       "APLICA√á√ïES:\n",
       "- Desenvolvimento web (Django, Flask)\n",
       "- Intelig√™ncia artificial (TensorFlow, PyTorch)\n",
       "- Ci√™ncia de dados (Pandas, NumPy)\n",
       "- Automa√ß√£o e scripts\n",
       "- Jogos (Pygame)\n",
       "\n",
       "JAVASCRIPT - LINGUAGEM WEB\n",
       "\n",
       "JavaScript √© uma linguagem de programa√ß√£o interpretada, criada por Brendan Eich em 1995.\n",
       "√â a linguagem principal para desenvolvimento web frontend, permitindo criar p√°ginas\n",
       "interativas e din√¢micas.\n",
       "\n",
       "CARACTER√çSTICAS:\n",
       "- Linguagem de tipagem din√¢mica\n",
       "- Executada no navegador\n",
       "- Suporte a programa√ß√£o funcional\n",
       "- Ecossistema rico (Node.js, React, Vue)\n",
       "\n",
       "APLICA√á√ïES:\n",
       "- Desenvolvimento frontend\n",
       "- Aplica√ß√µes web (React, Angular, Vue)\n",
       "- Backend (Node.js)\n",
       "- Aplica√ß√µes m√≥veis (React Native)\n",
       "- Jogos web\n",
       "\n",
       "INTELIG√äNCIA ARTIFICIAL - CONCEITOS\n",
       "\n",
       "Intelig√™ncia Artificial (IA) √© um campo da computa√ß√£o que busca criar sistemas\n",
       "capazes de realizar tarefas que normalmente requerem intelig√™ncia humana.\n",
       "\n",
       "TIPOS DE IA:\n",
       "- IA Fraca (Narrow AI): Especializada em uma tarefa\n",
       "- IA Forte (AGI): Intelig√™ncia geral como humanos\n",
       "- Machine Learning: Aprendizado autom√°tico\n",
       "- Deep Learning: Redes neurais profundas\n",
       "\n",
       "APLICA√á√ïES PR√ÅTICAS:\n",
       "- Assistentes virtuais (Siri, Alexa)\n",
       "- Reconhecimento de imagem\n",
       "- Processamento de linguagem natural\n",
       "- Carros aut√¥nomos\n",
       "- Recomenda√ß√£o de produtos\n",
       "\n",
       "LANGCHAIN - FRAMEWORK DE IA\n",
       "\n",
       "LangChain √© um framework para desenvolvimento de aplica√ß√µes de IA que permite\n",
       "conectar diferentes modelos de linguagem e ferramentas de forma modular.\n",
       "\n",
       "COMPONENTES PRINCIPAIS:\n",
       "- Prompts: Templates para comunica√ß√£o com IA\n",
       "- Chains: Sequ√™ncias de opera√ß√µes\n",
       "- Memory: Armazenamento de contexto\n",
       "- Agents: IA que pode usar ferramentas\n",
       "- Document Loaders: Carregamento de documentos\n",
       "\n",
       "CASOS DE USO:\n",
       "- Chatbots inteligentes\n",
       "- An√°lise de documentos\n",
       "- Automa√ß√£o de tarefas\n",
       "- Sistemas de recomenda√ß√£o\n",
       "- Assistentes pessoais\n",
       "\"\"\"\n",
       "\n",
       "# Salvando a base de conhecimento\n",
       "with open('base_conhecimento_tech.txt', 'w', encoding='utf-8') as file:\n",
       "    file.write(conhecimento_tech)\n",
       "\n",
       "print(\"üìö Base de conhecimento criada!\")\n",
       "print(\"ÔøΩÔøΩ Cont√©m informa√ß√µes sobre Python, JavaScript, IA e LangChain\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Criando Sistema RAG B√°sico**\n",
       "\n",
       "Vamos criar um sistema RAG que pode consultar nossa base de conhecimento:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Criando sistema RAG b√°sico\n",
       "# Como criar um estudante que consulta livros\n",
       "\n",
       "# Carregando e dividindo documentos\n",
       "loader = TextLoader('base_conhecimento_tech.txt', encoding='utf-8')\n",
       "documentos = loader.load()\n",
       "\n",
       "# Dividindo em peda√ßos menores\n",
       "text_splitter = RecursiveCharacterTextSplitter(\n",
       "    chunk_size=1000,\n",
       "    chunk_overlap=200\n",
       ")\n",
       "textos_divididos = text_splitter.split_documents(documentos)\n",
       "\n",
       "print(f\"üìÑ Documentos divididos em {len(textos_divididos)} peda√ßos\")\n",
       "\n",
       "# Criando embeddings e vector store\n",
       "embeddings = OpenAIEmbeddings(\n",
       "    openai_api_key=os.getenv('OPENAI_API_KEY')  # Configure sua API key no Colab\n",
       ")\n",
       "\n",
       "vectorstore = Chroma.from_documents(\n",
       "    documents=textos_divididos,\n",
       "    embedding=embeddings,\n",
       "    collection_name=\"base_tech\"\n",
       ")\n",
       "\n",
       "print(\"üß† Vector store criado com sucesso!\")\n",
       "print(\"üîç Base de conhecimento pronta para consulta\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Criando chain RAG\n",
       "# Como conectar busca com gera√ß√£o de respostas\n",
       "\n",
       "rag_chain = RetrievalQA.from_chain_type(\n",
       "    llm=llm,\n",
       "    chain_type=\"stuff\",  # Tipo de chain para RAG\n",
       "    retriever=vectorstore.as_retriever(\n",
       "        search_kwargs={\"k\": 3}  # Busca os 3 documentos mais relevantes\n",
       "    ),\n",
       "    return_source_documents=True,  # Retorna os documentos usados\n",
       "    verbose=True\n",
       ")\n",
       "\n",
       "print(\"üîó Chain RAG criada!\")\n",
       "print(\"ü§ñ IA pronta para consultar a base de conhecimento\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Testando sistema RAG b√°sico\n",
       "# Vamos ver a IA consultando a base de conhecimento!\n",
       "\n",
       "print(\"ÔøΩÔøΩ TESTE: SISTEMA RAG B√ÅSICO\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "perguntas_teste = [\n",
       "    \"O que √© Python e para que serve?\",\n",
       "    \"Quais s√£o as caracter√≠sticas do JavaScript?\",\n",
       "    \"Como funciona a Intelig√™ncia Artificial?\",\n",
       "    \"O que √© LangChain e quais s√£o seus componentes?\",\n",
       "    \"Qual a diferen√ßa entre Python e JavaScript?\"\n",
       "]\n",
       "\n",
       "for i, pergunta in enumerate(perguntas_teste, 1):\n",
       "    print(f\"\\n‚ùì Pergunta {i}: {pergunta}\")\n",
       "    \n",
       "    try:\n",
       "        # Executando RAG\n",
       "        resultado = rag_chain({\"query\": pergunta})\n",
       "        \n",
       "        print(f\"ü§ñ Resposta: {resultado['result']}\")\n",
       "        print(f\"ÔøΩÔøΩ Documentos consultados: {len(resultado['source_documents'])}\")\n",
       "        \n",
       "        # Mostrando um dos documentos consultados\n",
       "        if resultado['source_documents']:\n",
       "            doc = resultado['source_documents'][0]\n",
       "            print(f\"üìñ Fonte: {doc.page_content[:100]}...\")\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"‚ùå Erro: {e}\")\n",
       "    \n",
       "    print(\"-\" * 40)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **Aula 7.2: RAG Avan√ßado - Otimizando as Respostas**\n",
       "\n",
       "### **Compara√ß√£o: Com vs Sem RAG**\n",
       "\n",
       "Vamos ver a diferen√ßa entre uma IA normal e uma IA com RAG:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Teste: IA sem RAG vs IA com RAG\n",
       "# Comparando respostas com e sem consulta √† base de conhecimento\n",
       "\n",
       "print(\"ÔøΩÔøΩ COMPARA√á√ÉO: COM vs SEM RAG\")\n",
       "print(\"=\" * 60)\n",
       "\n",
       "pergunta_teste = \"Quais s√£o as principais caracter√≠sticas do LangChain?\"\n",
       "\n",
       "print(f\"‚ùì Pergunta: {pergunta_teste}\")\n",
       "print(\"=\" * 60)\n",
       "\n",
       "# Teste 1: IA sem RAG (s√≥ conhecimento interno)\n",
       "print(\"\\n‚ùå SEM RAG (IA sem consulta):\")\n",
       "try:\n",
       "    resposta_sem_rag = llm.invoke([\n",
       "        HumanMessage(content=pergunta_teste)\n",
       "    ])\n",
       "    print(f\"ü§ñ Resposta: {resposta_sem_rag.content[:300]}...\")\n",
       "except Exception as e:\n",
       "    print(f\"‚ùå Erro: {e}\")\n",
       "\n",
       "print(\"\\n\" + \"-\" * 50)\n",
       "\n",
       "# Teste 2: IA com RAG (consultando base de conhecimento)\n",
       "print(\"\\n‚úÖ COM RAG (IA consultando base):\")\n",
       "try:\n",
       "    resultado_rag = rag_chain({\"query\": pergunta_teste})\n",
       "    print(f\"ü§ñ Resposta: {resultado_rag['result'][:300]}...\")\n",
       "    print(f\"üìÑ Baseada em {len(resultado_rag['source_documents'])} documentos\")\n",
       "except Exception as e:\n",
       "    print(f\"‚ùå Erro: {e}\")\n",
       "\n",
       "print(\"\\n\" + \"=\" * 60)\n",
       "print(\"üí° Diferen√ßa: RAG fornece respostas mais precisas e baseadas em dados espec√≠ficos!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Sistema de Suporte T√©cnico com RAG**\n",
       "\n",
       "Vamos criar um **sistema de suporte t√©cnico inteligente** que usa RAG:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Criando sistema de suporte t√©cnico com RAG\n",
       "# Como criar um suporte que sempre consulta a documenta√ß√£o\n",
       "\n",
       "from langchain.prompts import PromptTemplate\n",
       "from langchain.chains import LLMChain\n",
       "\n",
       "# Template para suporte t√©cnico\n",
       "template_suporte = PromptTemplate(\n",
       "    input_variables=[\"context\", \"question\"],\n",
       "    template=\"\"\"\n",
       "    Voc√™ √© um especialista em suporte t√©cnico muito atencioso.\n",
       "    \n",
       "    Use as seguintes informa√ß√µes da base de conhecimento para responder:\n",
       "    {context}\n",
       "    \n",
       "    PERGUNTA DO CLIENTE: {question}\n",
       "    \n",
       "    INSTRU√á√ïES:\n",
       "    - Responda de forma clara e did√°tica\n",
       "    - Use as informa√ß√µes da base de conhecimento\n",
       "    - Seja prestativo e atencioso\n",
       "    - Use linguagem informal como o Pedro Guth\n",
       "    - Se n√£o encontrar informa√ß√£o espec√≠fica, seja honesto\n",
       "    \"\"\"\n",
       ")\n",
       "\n",
       "# Chain de suporte\n",
       "chain_suporte = LLMChain(\n",
       "    llm=llm,\n",
       "    prompt=template_suporte\n",
       ")\n",
       "\n",
       "print(\"üõ†Ô∏è  Sistema de suporte t√©cnico criado!\")\n",
       "print(\"üìö Sempre consulta a base de conhecimento\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Fun√ß√£o para buscar contexto relevante\n",
       "def buscar_contexto(pergunta, k=2):\n",
       "    \"\"\"Busca documentos relevantes para a pergunta\"\"\"\n",
       "    try:\n",
       "        docs = vectorstore.similarity_search(pergunta, k=k)\n",
       "        contexto = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
       "        return contexto\n",
       "    except Exception as e:\n",
       "        return f\"Erro na busca: {e}\"\n",
       "\n",
       "# Testando sistema de suporte\n",
       "print(\"üõ†Ô∏è  TESTE: SISTEMA DE SUPORTE T√âCNICO\")\n",
       "print(\"=\" * 60)\n",
       "\n",
       "perguntas_suporte = [\n",
       "    \"Preciso aprender Python para trabalhar com IA. Por onde come√ßar?\",\n",
       "    \"Qual a diferen√ßa entre JavaScript e Python?\",\n",
       "    \"Como posso usar LangChain no meu projeto?\",\n",
       "    \"O que √© machine learning e como se relaciona com IA?\"\n",
       "]\n",
       "\n",
       "for i, pergunta in enumerate(perguntas_suporte, 1):\n",
       "    print(f\"\\nüë§ Cliente {i}: {pergunta}\")\n",
       "    \n",
       "    try:\n",
       "        # Buscando contexto relevante\n",
       "        contexto = buscar_contexto(pergunta)\n",
       "        \n",
       "        # Gerando resposta com contexto\n",
       "        resposta = chain_suporte.run({\n",
       "            \"context\": contexto,\n",
       "            \"question\": pergunta\n",
       "        })\n",
       "        \n",
       "        print(f\"üõ†Ô∏è  Suporte: {resposta[:400]}...\")\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"‚ùå Erro: {e}\")\n",
       "    \n",
       "    print(\"-\" * 40)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Chatbot com RAG - Sistema Completo**\n",
       "\n",
       "Agora vamos criar um **chatbot completo** que combina RAG com memory:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Criando chatbot com RAG e memory\n",
       "# Como criar um assistente que lembra e consulta documentos\n",
       "\n",
       "from langchain.memory import ConversationBufferMemory\n",
       "from langchain.chains import ConversationalRetrievalChain\n",
       "\n",
       "# Memory para o chatbot\n",
       "memory = ConversationBufferMemory(\n",
       "    memory_key=\"chat_history\",\n",
       "    return_messages=True\n",
       ")\n",
       "\n",
       "# Chatbot com RAG\n",
       "chatbot_rag = ConversationalRetrievalChain.from_llm(\n",
       "    llm=llm,\n",
       "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
       "    memory=memory,\n",
       "    return_source_documents=True,\n",
       "    verbose=True\n",
       ")\n",
       "\n",
       "print(\"ü§ñ Chatbot com RAG criado!\")\n",
       "print(\"üß† Combina busca de documentos com memory de conversa\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Testando chatbot com RAG\n",
       "# Vamos ver o assistente em a√ß√£o!\n",
       "\n",
       "print(\"ÔøΩÔøΩ TESTE: CHATBOT COM RAG\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "conversa = [\n",
       "    \"Ol√°! Sou desenvolvedor e quero aprender sobre IA.\",\n",
       "    \"O que voc√™ pode me contar sobre Python?\",\n",
       "    \"E como Python se relaciona com IA?\",\n",
       "    \"Voc√™ lembra o que eu disse sobre mim?\",\n",
       "    \"Pode me explicar o que √© LangChain?\"\n",
       "]\n",
       "\n",
       "for i, mensagem in enumerate(conversa, 1):\n",
       "    print(f\"\\nÔøΩÔøΩ Usu√°rio {i}: {mensagem}\")\n",
       "    \n",
       "    try:\n",
       "        # Executando chatbot\n",
       "        resultado = chatbot_rag({\"question\": mensagem})\n",
       "        \n",
       "        print(f\"ü§ñ Chatbot: {resultado['answer'][:300]}...\")\n",
       "        \n",
       "        if resultado['source_documents']:\n",
       "            print(f\"üìÑ Consultou {len(resultado['source_documents'])} documentos\")\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"‚ùå Erro: {e}\")\n",
       "    \n",
       "    print(\"-\" * 30)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Na Pr√°tica, Meu Consagrado!** üí™\n",
       "\n",
       "**O que aprendemos sobre RAG:**\n",
       "\n",
       "1. ‚úÖ **RAG B√°sico** - IA que consulta documentos antes de responder\n",
       "2. ‚úÖ **Compara√ß√£o** - Com vs sem RAG (diferen√ßa brutal!)\n",
       "3. ‚úÖ **Sistema de Suporte** - Suporte t√©cnico que sempre consulta documenta√ß√£o\n",
       "4. ‚úÖ **Chatbot Completo** - Combina RAG com memory\n",
       "\n",
       "### **Vantagens do RAG**\n",
       "\n",
       "- **Precis√£o**: Respostas baseadas em dados espec√≠ficos\n",
       "- **Atualiza√ß√£o**: Pode usar informa√ß√µes recentes\n",
       "- **Transpar√™ncia**: Mostra as fontes consultadas\n",
       "- **Flexibilidade**: Funciona com qualquer base de conhecimento\n",
       "\n",
       "### **Compara√ß√£o: Com vs Sem LangChain**\n",
       "\n",
       "**Sem LangChain**: Voc√™ teria que implementar RAG do zero\n",
       "**Com LangChain**: Tudo pronto, s√≥ configurar!\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Um diagrama mostrando o fluxo RAG (pergunta ‚Üí busca ‚Üí contexto ‚Üí resposta)\n",
       "\n",
       "**üéØ Pr√≥ximo m√≥dulo**: Vamos aprender sobre **Projetos Pr√°ticos** - como aplicar tudo que aprendemos!\n",
       "\n",
       "**ÔøΩÔøΩ Resumo do M√≥dulo 7**:\n",
       "- ‚úÖ RAG b√°sico e avan√ßado\n",
       "- ‚úÖ Sistemas de suporte inteligente\n",
       "- ‚úÖ Chatbots com base de conhecimento\n",
       "- ‚úÖ Otimiza√ß√£o de respostas\n",
       "\n",
       "**üîç Agora voc√™ sabe fazer IA que consulta documentos antes de responder!**"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }