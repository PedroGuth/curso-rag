{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# üß© **M√≥dulo 7: RAG Completo - Juntando Todos os Peda√ßos**\n",
       "\n",
       "> *Agora vamos montar o quebra-cabe√ßa completo e ver a m√°gica acontecer!*\n",
       "\n",
       "---\n",
       "\n",
       "## **Aula 7.1: O que √© RAG Completo e por que √© como um Chef Completo?**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas o que √© RAG Completo?**\n",
       "\n",
       "RAG Completo √© como um **chef experiente** que pega todos os ingredientes (documentos), prepara eles (embeddings), organiza na despensa (vector store), busca o que precisa (retrieval) e cozinha um prato delicioso (resposta)! üë®‚Äçüç≥\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Um chef cozinhando com todos os ingredientes organizados\n",
       "\n",
       "**Por que RAG Completo √© importante?**\n",
       "\n",
       "√â como a diferen√ßa entre:\n",
       "- ÔøΩÔøΩ **Ingredientes soltos**: Dif√≠cil de usar\n",
       "- üçΩÔ∏è **Prato completo**: Delicioso e funcional\n",
       "\n",
       "### **Analogia do Dia a Dia**\n",
       "\n",
       "RAG Completo √© como **montar um quebra-cabe√ßa gigante**:\n",
       "- üß© **Pe√ßas soltas**: Documentos, embeddings, vector stores\n",
       "- ÔøΩÔøΩÔ∏è **Imagem completa**: Sistema RAG funcionando perfeitamente\n",
       "- ÔøΩÔøΩ **Resultado**: Respostas precisas e confi√°veis\n",
       "\n",
       "**Sem RAG Completo** seria como ter todas as pe√ßas do quebra-cabe√ßa mas nunca mont√°-lo! üòÖ\n",
       "\n",
       "---\n",
       "\n",
       "### **Setup Inicial - Preparando o Terreno**\n",
       "\n",
       "**‚ö†Ô∏è IMPORTANTE**: Se voc√™ n√£o executou o notebook `00_setup_colab.ipynb` primeiro, execute a c√©lula abaixo para instalar as depend√™ncias."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üöÄ SETUP GRATUITO PARA COLAB\n",
       "# Execute esta c√©lula primeiro para configurar o ambiente!\n",
       "\n",
       "# Instalando depend√™ncias (execute apenas se necess√°rio)\n",
       "!pip install langchain>=0.1.0\n",
       "!pip install langchain-community>=0.0.10\n",
       "!pip install langchain-core>=0.1.0\n",
       "!pip install python-dotenv>=1.0.0\n",
       "!pip install huggingface_hub>=0.19.0\n",
       "!pip install sentence-transformers>=2.2.0\n",
       "!pip install chromadb>=0.4.0\n",
       "!pip install faiss-cpu>=1.7.0\n",
       "!pip install numpy>=1.24.0\n",
       "!pip install pandas>=2.0.0\n",
       "!pip install matplotlib>=3.5.0\n",
       "!pip install scikit-learn>=1.3.0\n",
       "\n",
       "print(\"‚úÖ Depend√™ncias instaladas com sucesso!\")\n",
       "print(\"ÔøΩÔøΩ Pronto para montar o RAG completo!\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ IMPORTA√á√ïES PARA RAG COMPLETO\n",
       "import os\n",
       "from dotenv import load_dotenv\n",
       "\n",
       "# LangChain\n",
       "from langchain_community.llms import HuggingFaceHub\n",
       "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
       "from langchain_community.vectorstores import Chroma\n",
       "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
       "from langchain.chains import RetrievalQA\n",
       "from langchain.chains import ConversationalRetrievalChain\n",
       "from langchain.memory import ConversationBufferMemory\n",
       "from langchain.schema import Document\n",
       "\n",
       "# Utilit√°rios\n",
       "import numpy as np\n",
       "import pandas as pd\n",
       "\n",
       "print(\"‚úÖ Bibliotecas importadas com sucesso!\")\n",
       "print(\"üß© Pronto para criar o RAG completo!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Exemplo Pr√°tico - Criando Nosso Primeiro RAG Completo**\n",
       "\n",
       "Vamos criar um sistema RAG completo sobre tecnologia. √â como montar um quebra-cabe√ßa sobre o futuro! üöÄ"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ EXEMPLO PR√ÅTICO: CRIANDO NOSSO PRIMEIRO RAG COMPLETO\n",
       "\n",
       "# Criando base de conhecimento sobre tecnologia\n",
       "base_conhecimento_tech = [\n",
       "    \"Intelig√™ncia Artificial (IA) √© um campo da computa√ß√£o que busca criar sistemas capazes de realizar tarefas que normalmente requerem intelig√™ncia humana. A IA combina ci√™ncia da computa√ß√£o, matem√°tica, psicologia e outras disciplinas.\",\n",
       "    \n",
       "    \"Machine Learning √© um subcampo da IA que permite aos computadores aprender sem serem explicitamente programados. Os algoritmos identificam padr√µes nos dados para fazer previs√µes ou tomar decis√µes.\",\n",
       "    \n",
       "    \"Deep Learning √© um subcampo do Machine Learning que usa redes neurais com m√∫ltiplas camadas para aprender representa√ß√µes complexas dos dados. Requer grandes quantidades de dados e poder computacional significativo.\",\n",
       "    \n",
       "    \"RAG (Retrieval Augmented Generation) √© uma t√©cnica que combina busca de informa√ß√µes com gera√ß√£o de texto. Em vez de gerar respostas baseadas apenas no conhecimento pr√©vio, o RAG consulta documentos relevantes antes de responder.\",\n",
       "    \n",
       "    \"ChatGPT √© um modelo de linguagem desenvolvido pela OpenAI. √â baseado na arquitetura GPT (Generative Pre-trained Transformer) e √© capaz de gerar texto humano de alta qualidade.\",\n",
       "    \n",
       "    \"Blockchain √© uma tecnologia de registro distribu√≠do que permite transa√ß√µes seguras e transparentes sem a necessidade de intermedi√°rios. √â a base das criptomoedas como Bitcoin.\",\n",
       "    \n",
       "    \"Cloud Computing √© a entrega de servi√ßos de computa√ß√£o pela internet. Inclui servidores, armazenamento, bancos de dados, redes, software e intelig√™ncia artificial.\",\n",
       "    \n",
       "    \"Internet das Coisas (IoT) √© a rede de dispositivos f√≠sicos conectados √† internet. Esses dispositivos coletam e trocam dados, permitindo automa√ß√£o e monitoramento inteligente.\",\n",
       "    \n",
       "    \"Realidade Virtual (VR) √© uma tecnologia que cria ambientes simulados em 3D. Os usu√°rios podem interagir com esses ambientes usando dispositivos especiais como √≥culos VR.\",\n",
       "    \n",
       "    \"Realidade Aumentada (AR) √© uma tecnologia que sobrep√µe informa√ß√µes digitais ao mundo real. Diferente da VR, a AR n√£o substitui o ambiente real, mas o complementa.\",\n",
       "    \n",
       "    \"5G √© a quinta gera√ß√£o de tecnologia de rede m√≥vel. Oferece velocidades muito mais r√°pidas, menor lat√™ncia e maior capacidade de conex√£o simult√¢nea de dispositivos.\",\n",
       "    \n",
       "    \"Ciberseguran√ßa √© a prote√ß√£o de sistemas, redes e programas contra ataques digitais. Inclui medidas para proteger dados, prevenir ataques e garantir a privacidade dos usu√°rios.\",\n",
       "    \n",
       "    \"Big Data refere-se a conjuntos de dados muito grandes e complexos que n√£o podem ser processados por m√©todos tradicionais. Requer t√©cnicas especiais de an√°lise e processamento.\",\n",
       "    \n",
       "    \"Automa√ß√£o √© o uso de tecnologia para executar tarefas sem interven√ß√£o humana. Pode incluir rob√≥tica, software e sistemas inteligentes que substituem ou complementam o trabalho humano.\",\n",
       "    \n",
       "    \"Computa√ß√£o Qu√¢ntica √© um tipo de computa√ß√£o que usa fen√¥menos qu√¢nticos como superposi√ß√£o e entrela√ßamento. Tem potencial para resolver problemas que s√£o imposs√≠veis para computadores cl√°ssicos.\"\n",
       "]\n",
       "\n",
       "print(f\"üìö Base de conhecimento criada com {len(base_conhecimento_tech)} documentos sobre tecnologia\")\n",
       "print(\"ÔøΩÔøΩ Pronto para montar nosso RAG completo!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **Aula 7.2: Passo 1 - Preparando os Ingredientes (Document Loaders e Text Splitting)**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas como preparar os ingredientes?**\n",
       "\n",
       "Primeiro, vamos preparar nossos \"ingredientes\" - transformar os documentos em peda√ßos gerenci√°veis! ü•ï\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Um chef cortando e preparando ingredientes"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ PASSO 1: PREPARANDO OS INGREDIENTES\n",
       "\n",
       "# Criando documentos do LangChain\n",
       "documentos = [Document(page_content=texto, metadata={\"fonte\": \"base_tech\", \"id\": i}) \n",
       "              for i, texto in enumerate(base_conhecimento_tech)]\n",
       "\n",
       "print(f\"üìÑ {len(documentos)} documentos criados\")\n",
       "\n",
       "# Dividindo em peda√ßos menores (Text Splitting)\n",
       "text_splitter = RecursiveCharacterTextSplitter(\n",
       "    chunk_size=200,      # Tamanho de cada peda√ßo\n",
       "    chunk_overlap=50,    # Sobreposi√ß√£o entre peda√ßos\n",
       "    separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
       ")\n",
       "\n",
       "# Aplicando o splitter\n",
       "textos_divididos = text_splitter.split_documents(documentos)\n",
       "\n",
       "print(f\"üî™ Documentos divididos em {len(textos_divididos)} peda√ßos\")\n",
       "print(f\"üìè Tamanho m√©dio: {sum(len(doc.page_content) for doc in textos_divididos) / len(textos_divididos):.0f} caracteres\")\n",
       "\n",
       "# Mostrando alguns peda√ßos\n",
       "print(\"\\nüçΩÔ∏è Primeiros 3 peda√ßos preparados:\")\n",
       "for i, doc in enumerate(textos_divididos[:3], 1):\n",
       "    print(f\"\\n--- Peda√ßo {i} ---\")\n",
       "    print(doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content)\n",
       "\n",
       "print(\"\\n‚úÖ Ingredientes preparados com sucesso!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **Aula 7.3: Passo 2 - Criando os Embeddings (Transformando em N√∫meros)**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas como transformar texto em n√∫meros?**\n",
       "\n",
       "Agora vamos transformar nossos textos em **coordenadas num√©ricas** que o computador entende! üßÆ\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Palavras sendo transformadas em coordenadas no espa√ßo"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üßÆ PASSO 2: CRIANDO OS EMBEDDINGS\n",
       "\n",
       "# Criando modelo de embeddings\n",
       "embeddings = HuggingFaceEmbeddings(\n",
       "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
       "    model_kwargs={'device': 'cpu'}\n",
       ")\n",
       "\n",
       "print(\"üß† Modelo de embeddings carregado!\")\n",
       "print(f\"üìê Dimens√£o dos embeddings: {embeddings.client.get_sentence_embedding_dimension()}\")\n",
       "\n",
       "# Testando embeddings com algumas frases\n",
       "frases_teste = [\n",
       "    \"Intelig√™ncia Artificial √© incr√≠vel\",\n",
       "    \"Machine Learning √© um subcampo da IA\",\n",
       "    \"O tempo est√° chuvoso hoje\"\n",
       "]\n",
       "\n",
       "print(\"\\nüß™ Testando embeddings:\")\n",
       "for i, frase in enumerate(frases_teste, 1):\n",
       "    embedding = embeddings.embed_query(frase)\n",
       "    print(f\"   {i}. '{frase}' -> {len(embedding)} n√∫meros\")\n",
       "\n",
       "print(\"\\n‚úÖ Embeddings funcionando perfeitamente!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **Aula 7.4: Passo 3 - Organizando na Despensa (Vector Store)**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas onde guardar todos esses n√∫meros?**\n",
       "\n",
       "Agora vamos organizar nossos embeddings em uma **\"despensa inteligente\"** onde √© f√°cil encontrar o que precisamos! ÔøΩÔøΩÔ∏è\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Uma despensa organizada com ingredientes bem arrumados"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üóÑÔ∏è PASSO 3: ORGANIZANDO NA DESPENSA (VECTOR STORE)\n",
       "\n",
       "# Criando vector store com Chroma\n",
       "vectorstore = Chroma.from_documents(\n",
       "    documents=textos_divididos,\n",
       "    embedding=embeddings,\n",
       "    collection_name=\"base_tecnologia\"\n",
       ")\n",
       "\n",
       "print(\"üóÑÔ∏è Vector store criado com sucesso!\")\n",
       "print(f\"üìö {len(textos_divididos)} documentos indexados\")\n",
       "print(f\"ÔøΩÔøΩ Embeddings organizados por similaridade\")\n",
       "\n",
       "# Testando a busca no vector store\n",
       "print(\"\\nüîç Testando busca no vector store:\")\n",
       "pergunta_teste = \"O que √© intelig√™ncia artificial?\"\n",
       "docs_encontrados = vectorstore.similarity_search(pergunta_teste, k=2)\n",
       "\n",
       "print(f\"‚ùì Pergunta: {pergunta_teste}\")\n",
       "print(\"üìÑ Documentos encontrados:\")\n",
       "for i, doc in enumerate(docs_encontrados, 1):\n",
       "    print(f\"   {i}. {doc.page_content[:100]}...\")\n",
       "\n",
       "print(\"\\n‚úÖ Despensa organizada e funcionando!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **Aula 7.5: Passo 4 - Configurando o Chef (LLM)**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas quem vai cozinhar a resposta?**\n",
       "\n",
       "Agora vamos configurar nosso **\"chef inteligente\"** que vai pegar os ingredientes e criar a resposta final! üë®‚Äçüç≥\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Um chef cozinhando com ingredientes frescos"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# ÔøΩÔøΩ‚Äçüç≥ PASSO 4: CONFIGURANDO O CHEF (LLM)\n",
       "\n",
       "# Configurando o LLM (nosso chef)\n",
       "def get_llm_colab():\n",
       "    \"\"\"Retorna o melhor LLM dispon√≠vel no Colab\"\"\"\n",
       "    \n",
       "    # Tentativa 1: OpenAI (se tiver API key)\n",
       "    try:\n",
       "        from langchain_openai import ChatOpenAI\n",
       "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
       "        if api_key:\n",
       "            return ChatOpenAI(\n",
       "                model=\"gpt-3.5-turbo\",\n",
       "                temperature=0.7,\n",
       "                api_key=api_key\n",
       "            )\n",
       "    except:\n",
       "        pass\n",
       "    \n",
       "    # Tentativa 2: Hugging Face (gratuito)\n",
       "    try:\n",
       "        token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
       "        if token:\n",
       "            return HuggingFaceHub(\n",
       "                repo_id=\"google/flan-t5-base\",\n",
       "                model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
       "            )\n",
       "    except:\n",
       "        pass\n",
       "    \n",
       "    # Fallback: LLM simples\n",
       "    return HuggingFaceHub(\n",
       "        repo_id=\"google/flan-t5-base\",\n",
       "        model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
       "    )\n",
       "\n",
       "# Configurando o LLM\n",
       "llm = get_llm_colab()\n",
       "\n",
       "print(f\"üë®‚Äçüç≥ Chef configurado: {type(llm).__name__}\")\n",
       "print(\"ÔøΩÔøΩ Pronto para cozinhar respostas deliciosas!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **Aula 7.6: Passo 5 - Montando o RAG Completo**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas como juntar tudo?**\n",
       "\n",
       "Agora vamos **montar o quebra-cabe√ßa completo** - juntar todos os peda√ßos para criar nosso sistema RAG! üß©\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Um quebra-cabe√ßa sendo montado pe√ßa por pe√ßa"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üß© PASSO 5: MONTANDO O RAG COMPLETO\n",
       "\n",
       "# Criando o sistema RAG completo\n",
       "rag_system = RetrievalQA.from_chain_type(\n",
       "    llm=llm,\n",
       "    chain_type=\"stuff\",\n",
       "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
       "    return_source_documents=True,\n",
       "    verbose=True\n",
       ")\n",
       "\n",
       "print(\"üß© Sistema RAG completo montado!\")\n",
       "print(\"üéØ Pronto para responder perguntas sobre tecnologia!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Teste R√°pido - Nosso RAG em A√ß√£o!**\n",
       "\n",
       "Vamos testar nosso sistema RAG completo com algumas perguntas:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üß™ TESTE R√ÅPIDO: NOSSO RAG EM A√á√ÉO!\n",
       "\n",
       "# Perguntas para testar o sistema\n",
       "perguntas_teste = [\n",
       "    \"O que √© intelig√™ncia artificial?\",\n",
       "    \"Como funciona o machine learning?\",\n",
       "    \"Qual a diferen√ßa entre VR e AR?\",\n",
       "    \"O que √© blockchain?\",\n",
       "    \"Como funciona o 5G?\"\n",
       "]\n",
       "\n",
       "print(\"ÔøΩÔøΩ TESTANDO NOSSO RAG COMPLETO\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "for i, pergunta in enumerate(perguntas_teste, 1):\n",
       "    print(f\"\\n‚ùì Pergunta {i}: {pergunta}\")\n",
       "    print(\"-\" * 40)\n",
       "    \n",
       "    try:\n",
       "        # Executando o RAG\n",
       "        resultado = rag_system({\"query\": pergunta})\n",
       "        \n",
       "        print(f\"ü§ñ Resposta: {resultado['result'][:200]}...\")\n",
       "        \n",
       "        if resultado['source_documents']:\n",
       "            print(f\"ÔøΩÔøΩ Fontes consultadas: {len(resultado['source_documents'])} documentos\")\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"‚ùå Erro: {e}\")\n",
       "    \n",
       "    print(\"-\" * 40)\n",
       "\n",
       "print(\"\\nüéâ Sistema RAG funcionando perfeitamente!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## **Aula 7.7: RAG Conversacional - Com Mem√≥ria**\n",
       "\n",
       "---\n",
       "\n",
       "### **T√°, mas e se eu quiser conversar com o RAG?**\n",
       "\n",
       "Vamos criar um RAG **conversacional** que lembra do que voc√™ falou antes! √â como conversar com um amigo que tem uma biblioteca na cabe√ßa! ÔøΩÔøΩÔ∏è\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Duas pessoas conversando, uma com uma biblioteca flutuando sobre a cabe√ßa"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üó£Ô∏è RAG CONVERSACIONAL - COM MEM√ìRIA\n",
       "\n",
       "# Criando mem√≥ria para o RAG\n",
       "memory = ConversationBufferMemory(\n",
       "    memory_key=\"chat_history\",\n",
       "    return_messages=True\n",
       ")\n",
       "\n",
       "# Criando RAG conversacional\n",
       "rag_conversacional = ConversationalRetrievalChain.from_llm(\n",
       "    llm=llm,\n",
       "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
       "    memory=memory,\n",
       "    return_source_documents=True,\n",
       "    verbose=True\n",
       ")\n",
       "\n",
       "print(\"üó£Ô∏è RAG conversacional criado!\")\n",
       "print(\"üß† Com mem√≥ria para lembrar da conversa!\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üß™ TESTANDO RAG CONVERSACIONAL\n",
       "\n",
       "print(\"ÔøΩÔøΩÔ∏è TESTANDO RAG CONVERSACIONAL\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "# Simulando uma conversa\n",
       "conversa = [\n",
       "    \"O que √© intelig√™ncia artificial?\",\n",
       "    \"E como ela se relaciona com machine learning?\",\n",
       "    \"Voc√™ pode me dar exemplos pr√°ticos?\",\n",
       "    \"Lembra do que eu perguntei sobre IA?\"\n",
       "]\n",
       "\n",
       "for i, mensagem in enumerate(conversa, 1):\n",
       "    print(f\"\\nÔøΩÔøΩ Usu√°rio {i}: {mensagem}\")\n",
       "    print(\"-\" * 40)\n",
       "    \n",
       "    try:\n",
       "        # Executando RAG conversacional\n",
       "        resultado = rag_conversacional({\"question\": mensagem})\n",
       "        \n",
       "        print(f\"ü§ñ RAG: {resultado['answer'][:200]}...\")\n",
       "        \n",
       "        if resultado['source_documents']:\n",
       "            print(f\"üìÑ Consultou {len(resultado['source_documents'])} documentos\")\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"‚ùå Erro: {e}\")\n",
       "    \n",
       "    print(\"-\" * 40)\n",
       "\n",
       "print(\"\\nÔøΩÔøΩ RAG conversacional funcionando perfeitamente!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Desafio do M√≥dulo - Criando um RAG Personalizado**\n",
       "\n",
       "Vamos criar um RAG personalizado que pode ser adaptado para diferentes dom√≠nios:"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# üéØ DESAFIO: CRIANDO UM RAG PERSONALIZADO\n",
       "\n",
       "class RAGPersonalizado:\n",
       "    \"\"\"Sistema RAG personalizado e flex√≠vel\"\"\"\n",
       "    \n",
       "    def __init__(self, nome_dominio=\"tecnologia\"):\n",
       "        self.nome_dominio = nome_dominio\n",
       "        self.vectorstore = None\n",
       "        self.llm = None\n",
       "        self.rag_system = None\n",
       "        \n",
       "    def configurar_llm(self):\n",
       "        \"\"\"Configura o LLM\"\"\"\n",
       "        self.llm = get_llm_colab()\n",
       "        print(f\"üë®‚Äçüç≥ Chef configurado para {self.nome_dominio}\")\n",
       "        \n",
       "    def carregar_documentos(self, documentos):\n",
       "        \"\"\"Carrega e processa documentos\"\"\"\n",
       "        # Criando documentos do LangChain\n",
       "        docs = [Document(page_content=texto, metadata={\"dominio\": self.nome_dominio, \"id\": i}) \n",
       "                for i, texto in enumerate(documentos)]\n",
       "        \n",
       "        # Dividindo em peda√ßos\n",
       "        text_splitter = RecursiveCharacterTextSplitter(\n",
       "            chunk_size=200,\n",
       "            chunk_overlap=50\n",
       "        )\n",
       "        \n",
       "        textos_divididos = text_splitter.split_documents(docs)\n",
       "        \n",
       "        # Criando embeddings e vector store\n",
       "        embeddings = HuggingFaceEmbeddings(\n",
       "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
       "        )\n",
       "        \n",
       "        self.vectorstore = Chroma.from_documents(\n",
       "            documents=textos_divididos,\n",
       "            embedding=embeddings,\n",
       "            collection_name=f\"base_{self.nome_dominio}\"\n",
       "        )\n",
       "        \n",
       "        print(f\"üìö {len(textos_divididos)} documentos carregados para {self.nome_dominio}\")\n",
       "        \n",
       "    def criar_sistema_rag(self):\n",
       "        \"\"\"Cria o sistema RAG completo\"\"\"\n",
       "        self.rag_system = RetrievalQA.from_chain_type(\n",
       "            llm=self.llm,\n",
       "            chain_type=\"stuff\",\n",
       "            retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
       "            return_source_documents=True\n",
       "        )\n",
       "        \n",
       "        print(f\"ÔøΩÔøΩ Sistema RAG criado para {self.nome_dominio}\")\n",
       "        \n",
       "    def perguntar(self, pergunta):\n",
       "        \"\"\"Faz uma pergunta ao sistema RAG\"\"\"\n",
       "        if not self.rag_system:\n",
       "            return \"Sistema RAG n√£o configurado!\"\n",
       "        \n",
       "        try:\n",
       "            resultado = self.rag_system({\"query\": pergunta})\n",
       "            return resultado['result']\n",
       "        except Exception as e:\n",
       "            return f\"Erro: {e}\"\n",
       "\n",
       "# Testando o RAG personalizado\n",
       "print(\"üéØ TESTANDO RAG PERSONALIZADO\")\n",
       "print(\"=\" * 50)\n",
       "\n",
       "# Criando RAG para tecnologia\n",
       "rag_tech = RAGPersonalizado(\"tecnologia\")\n",
       "rag_tech.configurar_llm()\n",
       "rag_tech.carregar_documentos(base_conhecimento_tech)\n",
       "rag_tech.criar_sistema_rag()\n",
       "\n",
       "# Testando\n",
       "pergunta_teste = \"O que √© deep learning?\"\n",
       "print(f\"\\n‚ùì Pergunta: {pergunta_teste}\")\n",
       "resposta = rag_tech.perguntar(pergunta_teste)\n",
       "print(f\"ü§ñ Resposta: {resposta[:200]}...\")\n",
       "\n",
       "print(\"\\nüéâ RAG personalizado funcionando perfeitamente!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### **Na Pr√°tica, Meu Consagrado!** üí™\n",
       "\n",
       "**O que constru√≠mos no RAG Completo:**\n",
       "\n",
       "1. ‚úÖ **Sistema RAG B√°sico** - Funcionando perfeitamente\n",
       "2. ‚úÖ **RAG Conversacional** - Com mem√≥ria de conversa\n",
       "3. ‚úÖ **RAG Personalizado** - Adapt√°vel para qualquer dom√≠nio\n",
       "4. ‚úÖ **Sistema Completo** - Do documento √† resposta final\n",
       "\n",
       "### **Vantagens do RAG Completo**\n",
       "\n",
       "- **Precis√£o**: Respostas baseadas em documentos reais\n",
       "- **Flexibilidade**: Adapt√°vel para qualquer dom√≠nio\n",
       "- **Mem√≥ria**: Lembra do contexto da conversa\n",
       "- **Escalabilidade**: Pode lidar com grandes volumes de dados\n",
       "- **Confiabilidade**: Reduz alucina√ß√µes da IA\n",
       "\n",
       "### **Pr√≥ximos Passos**\n",
       "\n",
       "**üñºÔ∏è Sugest√£o de imagem**: Fluxo completo do RAG mostrando todos os componentes funcionando juntos\n",
       "\n",
       "**üéØ Pr√≥ximo m√≥dulo**: Vamos criar **Projetos Pr√°ticos** - aplica√ß√µes reais do RAG!\n",
       "\n",
       "**ÔøΩÔøΩ Resumo do M√≥dulo 7**:\n",
       "- ‚úÖ Montamos o RAG completo passo a passo\n",
       "- ‚úÖ Criamos sistemas b√°sicos e conversacionais\n",
       "- ‚úÖ Desenvolvemos RAG personalizado\n",
       "- ‚úÖ Testamos com exemplos reais\n",
       "\n",
       "**üöÄ Agora voc√™ tem um sistema RAG completo e funcional!**\n",
       "\n",
       "---\n",
       "\n",
       "**ÔøΩÔøΩ Dica do Pedro**: O RAG √© como um quebra-cabe√ßa - cada pe√ßa √© importante, mas o resultado final √© muito maior que a soma das partes!\n",
       "\n",
       "**üöÄ Pr√≥ximo m√≥dulo**: Projetos Pr√°ticos - Colocando a m√£o na massa! üõ†Ô∏è"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }